{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# tqdm.pandas()\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "# bucket = os.environ.get(\"BUCKET_NAME\", \" \")\n",
    "# raw_data_folder = os.environ.get(\"RAW_DATA\", \" \")\n",
    "\n",
    "s3client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=aws-gcr-rs-sol-demo-ap-southeast-1-522244679887\n",
      "prefix='sample-data'\n",
      "file preparation: download src key sample-data/system/action-data/action.csv to dst key info/action.csv\n",
      "file preparation: download src key sample-data/system/item-data/item.csv to dst key info/item.csv\n",
      "file preparation: download src key sample-data/feature/content/inverted-list/news_id_news_feature_dict.pickle to dst key info/news_id_news_feature_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    print(\"upload s3://{}/{}\".format(bucket, key))\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        # return s3client.upload_fileobj(f, bucket, key)\n",
    "        return s3client.put_object(\n",
    "            ACL='bucket-owner-full-control',\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=f\n",
    "        )\n",
    "\n",
    "def write_str_to_s3(content, bucket, key):\n",
    "    print(\"write s3://{}/{}, content={}\".format(bucket, key, content))\n",
    "    s3client.put_object(Body=str(content).encode(\"utf8\"), Bucket=bucket, Key=key, ACL='bucket-owner-full-control')\n",
    "\n",
    "default_bucket = 'aws-gcr-rs-sol-demo-ap-southeast-1-522244679887'\n",
    "default_prefix = 'sample-data'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str, default=default_bucket)\n",
    "parser.add_argument('--prefix', type=str, default=default_prefix)\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "prefix = args.prefix\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "out_s3_path = \"s3://{}/{}/feature/content/inverted-list\".format(bucket, prefix)\n",
    "\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "# 行为/物品数据同步\n",
    "file_name_list = ['action.csv']\n",
    "s3_folder = '{}/system/action-data'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "file_name_list = ['item.csv']\n",
    "s3_folder = '{}/system/item-data'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "file_name_list = ['news_id_news_feature_dict.pickle']\n",
    "s3_folder = '{}/feature/content/inverted-list'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_load = open(\"info/news_id_news_feature_dict.pickle\", \"rb\")\n",
    "news_id_news_feature_dict = pickle.load(file_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_filter_item = pd.read_csv('info/item.csv',sep='_!_',names=['news_id','type_code','type','title','keywords','popularity','new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>type_code</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>popularity</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6423135627981619458</td>\n",
       "      <td>115</td>\n",
       "      <td>news_agriculture</td>\n",
       "      <td>为什么现在农村人很少到广州等地打工反而转战江苏浙江</td>\n",
       "      <td>一窝蜂,浙江,农村,江苏,广州</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6481778451320668430</td>\n",
       "      <td>106</td>\n",
       "      <td>news_house</td>\n",
       "      <td>河北衡水人天津集体户口在天津买房合适还是回衡水买呢</td>\n",
       "      <td>集体户口,天津,衡水,房价,大都市</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6487781016990646541</td>\n",
       "      <td>108</td>\n",
       "      <td>news_edu</td>\n",
       "      <td>襄阳的湖北交通大学将来会不会成为全国五大交校之一</td>\n",
       "      <td>襄阳,西南交通大学,交通大学,隆中,北京交通大学</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6492610374146195725</td>\n",
       "      <td>108</td>\n",
       "      <td>news_edu</td>\n",
       "      <td>如果哈工大不在哈尔滨而是在北京那现在哈工大会不会牛到飞起</td>\n",
       "      <td>北京,哈工大,哈尔滨</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6512940160806551816</td>\n",
       "      <td>109</td>\n",
       "      <td>news_tech</td>\n",
       "      <td>昆山江阴义乌哪个发展最好</td>\n",
       "      <td>义乌,物联网,昆山,江阴,小商品</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               news_id  type_code              type  \\\n",
       "0  6423135627981619458        115  news_agriculture   \n",
       "1  6481778451320668430        106        news_house   \n",
       "2  6487781016990646541        108          news_edu   \n",
       "3  6492610374146195725        108          news_edu   \n",
       "4  6512940160806551816        109         news_tech   \n",
       "\n",
       "                          title                  keywords  popularity  new  \n",
       "0     为什么现在农村人很少到广州等地打工反而转战江苏浙江           一窝蜂,浙江,农村,江苏,广州           0    0  \n",
       "1     河北衡水人天津集体户口在天津买房合适还是回衡水买呢         集体户口,天津,衡水,房价,大都市           3    0  \n",
       "2      襄阳的湖北交通大学将来会不会成为全国五大交校之一  襄阳,西南交通大学,交通大学,隆中,北京交通大学           9    1  \n",
       "3  如果哈工大不在哈尔滨而是在北京那现在哈工大会不会牛到飞起                北京,哈工大,哈尔滨           9    0  \n",
       "4                  昆山江阴义乌哪个发展最好          义乌,物联网,昆山,江阴,小商品           7    0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filter_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_filter_action = pd.read_csv('info/action.csv',sep='_!_',names=['user_id','news_id','timestamp','action_type','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552345461607367172</td>\n",
       "      <td>1618477588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552332581256299016</td>\n",
       "      <td>1618472565</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552130363123040771</td>\n",
       "      <td>1618467016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52a23654-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6475484594673025293</td>\n",
       "      <td>1618462187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52a238fc-9dc3-11eb-a364-acde48001122</td>\n",
       "      <td>6552277802022863374</td>\n",
       "      <td>1618468013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id              news_id   timestamp  \\\n",
       "0  52a23654-9dc3-11eb-a364-acde48001122  6552345461607367172  1618477588   \n",
       "1  52a23654-9dc3-11eb-a364-acde48001122  6552332581256299016  1618472565   \n",
       "2  52a23654-9dc3-11eb-a364-acde48001122  6552130363123040771  1618467016   \n",
       "3  52a23654-9dc3-11eb-a364-acde48001122  6475484594673025293  1618462187   \n",
       "4  52a238fc-9dc3-11eb-a364-acde48001122  6552277802022863374  1618468013   \n",
       "\n",
       "   action_type  action  \n",
       "0            1       0  \n",
       "1            1       0  \n",
       "2            1       0  \n",
       "3            1       1  \n",
       "4            1       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter_action.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6320151429650579970</td>\n",
       "      <td>1</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6418014704991469826</td>\n",
       "      <td>1</td>\n",
       "      <td>1.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6422082903882072321</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6422169653342109954</td>\n",
       "      <td>1</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6423135627981619458</td>\n",
       "      <td>1</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               news_id  action_type    action\n",
       "1  6320151429650579970            1  3.333333\n",
       "2  6418014704991469826            1  1.111111\n",
       "3  6422082903882072321            1  0.000000\n",
       "4  6422169653342109954            1  2.222222\n",
       "5  6423135627981619458            1  2.222222"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item_stats = df_filter_action[['news_id','action_type','action']]\n",
    "df_item_stats = df_item_stats.groupby(['news_id','action_type']).sum()\n",
    "df_item_stats = df_item_stats.reset_index()\n",
    "df_item_stats['action'] = df_item_stats['action'] / df_item_stats['action'].abs().max() * 10.0\n",
    "df_item_stats = df_item_stats.drop([0])\n",
    "df_item_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merge_result = pd.merge(df_filter_item, df_item_stats, on=\"news_id\", how=\"left\").drop(columns=['action_type'])\n",
    "pd_merge_result = pd_merge_result.fillna(0)\n",
    "# row_has_NaN = pd_merge_result.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_drop = pd_merge_result.drop(columns=['popularity']).rename(columns={\"action\":\"popularity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_drop.loc[pd_drop.new == 1, 'popularity'] = 10.0\n",
    "pd_drop.loc[pd_drop.new == 1, 'new'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dict_id_keywords for tfidf\n",
    "dict_keywords_id = {}\n",
    "for row in df_filter_item.iterrows():\n",
    "    item_row = row[1]\n",
    "    program_id = str(item_row['news_id'])\n",
    "    for kw in item_row['keywords'].split(','):\n",
    "        if kw not in dict_keywords_id.keys():\n",
    "            dict_keywords_id[kw] = [program_id]\n",
    "            continue\n",
    "        current_list = dict_keywords_id[kw]\n",
    "        current_list.append(program_id)\n",
    "        dict_keywords_id[kw].append(program_id)\n",
    "n_keyword_whole = len(dict_keywords_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(category_property):\n",
    "    if not category_property or str(category_property).lower() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    if not category_property:\n",
    "        return [None]\n",
    "    value = [item.strip() for item in category_property.split(',')]\n",
    "    keywords_tfidf = {}\n",
    "    for keyword in value:\n",
    "        current_score = 1 / len(value)*math.log(n_keyword_whole / len(dict_keywords_id[keyword]))\n",
    "        keywords_tfidf[keyword] = current_score\n",
    "    return keywords_tfidf\n",
    "        \n",
    "def get_category(category_property):\n",
    "    if not category_property or str(category_property).lower() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    if not category_property:\n",
    "        return [None]\n",
    "    return [item.strip().lower() for item in category_property.split(',')]\n",
    "            \n",
    "def get_single_item(item):\n",
    "    if not item or str(item).lower().strip() in ['nan', 'nr', '']:\n",
    "        return [None]\n",
    "    return [str(item).lower().strip()]\n",
    "\n",
    "def get_entities(news_id, news_id_news_feature_dict=news_id_news_feature_dict):\n",
    "    title_result = []\n",
    "    title_result.append(news_id_news_feature_dict[str(news_id)]['words'])\n",
    "    title_result.append(news_id_news_feature_dict[str(news_id)]['entities'])\n",
    "    return title_result\n",
    "\n",
    "def single_dict(raw_dict, feat, item_id):\n",
    "    if feat not in raw_dict.keys():\n",
    "        raw_dict[feat] = [item_id]\n",
    "    else:\n",
    "        current_list = raw_dict[feat]\n",
    "        current_list.append(item_id)\n",
    "        raw_dict[feat] = current_list\n",
    "\n",
    "def list_dict(raw_dict, feat_list, item_id):\n",
    "    for feat in feat_list:\n",
    "        single_dict(raw_dict, feat, item_id)\n",
    "\n",
    "def update_popularity(item_df, action_df):\n",
    "    pd_merge_result = pd.merge(item_df, action_df, on=\"news_id\", how=\"left\").drop(columns=['action_type'])\n",
    "    pd_merge_result = pd_merge_result.fillna(0)\n",
    "    df_update = pd_merge_result.drop(columns=['popularity']).rename(columns={\"action\":\"popularity\"})\n",
    "    df_update.loc[df_update.new == 1, 'popularity'] = 10.0\n",
    "    df_update.loc[df_update.new == 1, 'new'] = 0\n",
    "    return df_update\n",
    "        \n",
    "def sort_by_score(df):\n",
    "    logging.info(\"sort_by_score() enter, df.columns: {}\".format(df.columns))\n",
    "    df['popularity'].fillna(0, inplace=True)\n",
    "\n",
    "    df['popularity_log'] = np.log1p(df['popularity'])\n",
    "    popularity_log_max = df['popularity_log'].max()\n",
    "    popularity_log_min = df['popularity_log'].min()\n",
    "\n",
    "    df['popularity_scaled'] = ((df['popularity_log'] - popularity_log_min) / (\n",
    "            popularity_log_max - popularity_log_min)) * 10\n",
    "\n",
    "    df_sorted = df.sort_values(by='popularity_scaled', ascending=False)\n",
    "    \n",
    "    df_sorted = df_sorted.drop(\n",
    "        ['popularity_log', 'popularity_scaled'], axis=1)\n",
    "\n",
    "    logging.info(\"sort_by_score() return, df.columns: {}\".format(df_sorted.columns))\n",
    "    return df_sorted\n",
    "\n",
    "def get_bucket_key_from_s3_path(s3_path):\n",
    "    m = re.match(r\"s3://(.*?)/(.*)\", s3_path)\n",
    "    return m.group(1), m.group(2)\n",
    "\n",
    "def gen_pickle_files(df, action_df):\n",
    "    df_update = update_popularity(df, action_df)\n",
    "    df_sort = sort_by_score(df_update)\n",
    "    \n",
    "    news_id_news_property_dict = {}\n",
    "    news_type_news_ids_dict = {}\n",
    "    news_keywords_news_ids_dict = {}\n",
    "    news_entities_news_ids_dict = {}\n",
    "    news_words_news_ids_dict = {}\n",
    "    \n",
    "    for row in df_sort.iterrows():\n",
    "        item_row = row[1]\n",
    "        program_id = str(item_row['news_id'])\n",
    "        current_entities = get_entities(program_id)[1]\n",
    "        current_words = get_entities(program_id)[0]\n",
    "        program_dict = {\n",
    "            'title': get_single_item(item_row['title']),\n",
    "            'type': get_single_item(item_row['type']),\n",
    "            'keywords': get_category(item_row['keywords']),\n",
    "            'tfidf': get_tfidf(item_row['keywords']),\n",
    "            'entities': current_entities,\n",
    "            'words': current_words\n",
    "        }\n",
    "        news_id_news_property_dict[program_id] = program_dict\n",
    "        list_dict(news_type_news_ids_dict, program_dict['type'], program_id)\n",
    "        list_dict(news_keywords_news_ids_dict, program_dict['keywords'], program_id)\n",
    "        list_dict(news_entities_news_ids_dict, program_dict['entities'], program_id)\n",
    "        list_dict(news_words_news_ids_dict, program_dict['words'], program_id)\n",
    "\n",
    "    result_dict = {\n",
    "        'news_id_news_property_dict': news_id_news_property_dict,\n",
    "        'news_type_news_ids_dict': news_type_news_ids_dict,\n",
    "        'news_keywords_news_ids_dict': news_keywords_news_ids_dict,\n",
    "        'news_entities_news_ids_dict': news_entities_news_ids_dict,\n",
    "        'news_words_news_ids_dict': news_words_news_ids_dict\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = gen_pickle_files(df_filter_item, df_item_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_id_news_property_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_type_news_ids_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_keywords_news_ids_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_entities_news_ids_dict.pickle\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_words_news_ids_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "bucket, out_prefix = get_bucket_key_from_s3_path(out_s3_path)\n",
    "for dict_name, dict_val in rd.items():\n",
    "    file_name = f'{dict_name}.pickle'\n",
    "    # print(\"pickle =>\", file_name)\n",
    "    out_file = open(file_name, 'wb')\n",
    "    pickle.dump(dict_val, out_file)\n",
    "    out_file.close()\n",
    "    # s3_url = S3Uploader.upload(file_name, out_s3_path)\n",
    "    s3_url = write_to_s3(file_name, bucket, f'{out_prefix}/{file_name}')\n",
    "    logging.info(\"write {}\".format(s3_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 6423135627981619458 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0], 'words': [1, 2, 0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 0, 0]}\n",
      "k 6481778451320668430 v {'entities': [3, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [13, 0, 14, 15, 16, 17, 14, 18, 19, 20, 21, 22, 23, 18, 24, 0]}\n",
      "k 6487781016990646541 v {'entities': [0, 0, 5, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0], 'words': [25, 26, 27, 28, 29, 30, 31, 32, 31, 33, 34, 35, 36, 37, 0, 38]}\n",
      "k 6492610374146195725 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0], 'words': [39, 40, 32, 17, 41, 42, 43, 17, 44, 45, 2, 40, 31, 32, 31, 0]}\n",
      "k 6512940160806551816 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [46, 47, 48, 49, 50, 51, 52, 53, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6520157116831891716 v {'entities': [0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [54, 55, 56, 0, 57, 26, 58, 59, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6520548609547567367 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [60, 61, 62, 63, 64, 65, 66, 67, 61, 31, 68, 69, 70, 50, 71, 72]}\n",
      "k 6521960096736477700 v {'entities': [0, 0, 9, 0, 10, 0, 0, 11, 0, 12, 0, 0, 0, 0, 0, 0], 'words': [73, 74, 75, 76, 77, 78, 74, 79, 76, 80, 81, 74, 82, 83, 84, 85]}\n",
      "k 6528436786643862029 v {'entities': [0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [86, 87, 0, 44, 88, 89, 90, 91, 0, 92, 93, 94, 95, 26, 96, 0]}\n",
      "k 6531118183804305924 v {'entities': [0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [97, 17, 98, 29, 99, 26, 100, 101, 102, 103, 104, 43, 105, 106, 0, 0]}\n",
      "k 6537451193822609923 v {'entities': [0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [107, 108, 109, 110, 111, 108, 112, 113, 70, 114, 115, 116, 117, 118, 119, 120]}\n",
      "k 6539681813122515469 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [99, 121, 122, 123, 124, 69, 26, 0, 125, 126, 127, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k, v in news_id_news_feature_dict.items():\n",
    "    print(\"k {} v {}\".format(k,v))\n",
    "    if n > 10:\n",
    "        break\n",
    "    n = n + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
