{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from random import sample\n",
    "\n",
    "import boto3\n",
    "\n",
    "# tqdm.pandas()\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "# bucket = os.environ.get(\"BUCKET_NAME\", \" \")\n",
    "# raw_data_folder = os.environ.get(\"RAW_DATA\", \" \")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "s3client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=aws-gcr-rs-sol-demo-ap-southeast-1-522244679887\n",
      "prefix='sample-data'\n",
      "file preparation: download src key sample-data/feature/recommend-list/news/recall_batch_result.pickle to dst key info/recall_batch_result.pickle\n",
      "file preparation: download src key sample-data/feature/recommend-list/news/rank_batch_result.pickle to dst key info/rank_batch_result.pickle\n",
      "file preparation: download src key sample-data/feature/content/inverted-list/news_id_news_property_dict.pickle to dst key info/news_id_news_property_dict.pickle\n",
      "file preparation: download src key sample-data/feature/content/inverted-list/news_type_news_ids_dict.pickle to dst key info/news_type_news_ids_dict.pickle\n",
      "length of news_id v.s. news_property 2660\n",
      "length of news_type v.s. news_ids 15\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "\n",
    "\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    print(\"upload s3://{}/{}\".format(bucket, key))\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        # return s3client.upload_fileobj(f, bucket, key)\n",
    "        return s3client.put_object(\n",
    "            ACL='bucket-owner-full-control',\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=f\n",
    "        )\n",
    "\n",
    "def write_str_to_s3(content, bucket, key):\n",
    "    print(\"write s3://{}/{}, content={}\".format(bucket, key, content))\n",
    "    s3client.put_object(Body=str(content).encode(\"utf8\"), Bucket=bucket, Key=key, ACL='bucket-owner-full-control')\n",
    "\n",
    "default_bucket = 'aws-gcr-rs-sol-demo-ap-southeast-1-522244679887'\n",
    "default_prefix = 'sample-data'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str)\n",
    "parser.add_argument('--prefix', type=str)\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "prefix = args.prefix\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "out_s3_path = \"s3://{}/{}/feature/content/inverted-list\".format(bucket, prefix)\n",
    "\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "# recall & rank 结果加载\n",
    "file_name_list = ['recall_batch_result.pickle', 'rank_batch_result.pickle']\n",
    "s3_folder = '{}/feature/recommend-list/news'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# 倒排列表的pickle文件\n",
    "file_name_list = ['news_id_news_property_dict.pickle',\n",
    "                  'news_type_news_ids_dict.pickle']\n",
    "s3_folder = '{}/feature/content/inverted-list/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# filter配置项\n",
    "# file_name_list = ['filter_config.pickle']\n",
    "# s3_folder = '{}/model/filter/'.format(prefix)\n",
    "# sync_s3(file_name_list, s3_folder, local_folder)\n",
    "\n",
    "# 加载pickle文件\n",
    "file_to_load = open(\"info/news_id_news_property_dict.pickle\", \"rb\")\n",
    "dict_id_content = pickle.load(file_to_load)\n",
    "print(\"length of news_id v.s. news_property {}\".format(len(dict_id_content)))\n",
    "\n",
    "file_to_load = open(\"info/news_type_news_ids_dict.pickle\", \"rb\")\n",
    "dict_type_id = pickle.load(file_to_load)\n",
    "print(\"length of news_type v.s. news_ids {}\".format(len(dict_type_id)))\n",
    "\n",
    "# 加载filter配置\n",
    "filter_config = {}\n",
    "filter_config['category'] = ['news_story', 'news_culture', 'news_entertainment', 'news_sports', 'news_finance', 'news_house', 'news_car', 'news_edu', 'news_tech', 'news_military', 'news_travel', 'news_world', 'stock', 'news_agriculture', 'news_game']\n",
    "filter_config['category_diversity_count'] = 10\n",
    "\n",
    "# 加载recall结果\n",
    "file_to_load = open(\"info/recall_batch_result.pickle\", \"rb\")\n",
    "dict_recall_result = pickle.load(file_to_load)\n",
    "\n",
    "# 加载rank结果\n",
    "file_to_load = open(\"info/rank_batch_result.pickle\", \"rb\")\n",
    "dict_rank_result = pickle.load(file_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回结果格式设计：\n",
    "# item_id | recall_type | recall_score | rank_type | rank_score | filter_type | filter_score\n",
    "\n",
    "# recall_type: [运行时机]_[方法]_[位置]\n",
    "# [运行时机]: batch/online\n",
    "# [方法]: category/director/actor/language/level/year/review/photo/ub/portrai_xxx\n",
    "# [位置]: 数字[0-xxx]\n",
    "\n",
    "# recall_score: 召回得分，float型\n",
    "\n",
    "# rank_type: [运行时机]_[方法]_[位置]\n",
    "# [运行时机]: batch/online\n",
    "# [数据源头]: action/portrait\n",
    "# [方法]: deepfm/xgboost\n",
    "# [位置]: 数字[0-xxx]\n",
    "\n",
    "# rank_score: 排序得分，float型\n",
    "\n",
    "# filter_type: [运行时机]_[方法]_[位置]\n",
    "# [运行时机]: batch/online\n",
    "# [方法]: recommend/coldstart/disparity\n",
    "# [位置]: 数字[0-xxx]\n",
    "\n",
    "# filter_score: 过滤得分，float型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 8\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 8\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 8\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n",
      "ERROR:root:fail to find enough diversity candidate, need to find 10 but only find 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/recommend-list/news/filter_batch_result.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'Y4KPQXCRKF66XPW6',\n",
       "  'HostId': 'CNHW/sLNPdbLCbcMr5Pzf1fdf8AtoEEOJcnYgSeUkp+JL8RTwRKyh8CIT7SvUokczw4BBUNrRmg=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'CNHW/sLNPdbLCbcMr5Pzf1fdf8AtoEEOJcnYgSeUkp+JL8RTwRKyh8CIT7SvUokczw4BBUNrRmg=',\n",
       "   'x-amz-request-id': 'Y4KPQXCRKF66XPW6',\n",
       "   'date': 'Mon, 19 Apr 2021 07:55:32 GMT',\n",
       "   'etag': '\"fc8e023cbab51bef9e490a11d8b59573\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"fc8e023cbab51bef9e490a11d8b59573\"'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dict_pos(key, dict_var):\n",
    "    return list(dict_var.keys()).index(str(key))\n",
    "\n",
    "\n",
    "def calc_filter_score(recall_score, rank_score, recall_mt=None, rank_mt=None, recall_pos=None, rank_pos=None):\n",
    "    filter_score = min(1.0, recall_score / 40.0 + rank_score)\n",
    "    return round(filter_score, 2)\n",
    "\n",
    "\n",
    "def mt_construct(timing, mt, pos):\n",
    "    type_list = []\n",
    "    type_list.append(str(timing))\n",
    "    type_list.append(str(mt))\n",
    "    type_list.append(str(pos))\n",
    "    type_name = '_'.join(type_list)\n",
    "    return type_name\n",
    "\n",
    "\n",
    "def sort_and_fill_pos(filter_result):\n",
    "    sort_filter_result = dict(\n",
    "        sorted(filter_result.items(), key=lambda item: item[1][2], reverse=True))\n",
    "    filter_pos = 0\n",
    "    update_filter_result = dict()\n",
    "    for filter_id, filter_content in sort_filter_result.items():\n",
    "        current_trace = filter_content[3]\n",
    "        current_trace_split_list = current_trace.split('|')\n",
    "        current_filter_type = current_trace_split_list[4]\n",
    "        current_filter_type_split_list = current_filter_type.split('_')\n",
    "        update_filter_type_split_list = current_filter_type_split_list\n",
    "        update_filter_type_split_list[2] = str(filter_pos)\n",
    "        update_filter_type = '_'.join(update_filter_type_split_list)\n",
    "        update_trace_split_list = current_trace_split_list\n",
    "        update_trace_split_list[-2] = update_filter_type\n",
    "        update_trace = '|'.join(update_trace_split_list)\n",
    "        update_filter_content = filter_content\n",
    "        update_filter_content[3] = update_trace\n",
    "        #         print(\"update id {} trace {} type {}\".format(filter_id, update_trace,update_filter_type_split_list))\n",
    "        update_filter_result[str(filter_id)] = update_filter_content\n",
    "        # update filter pos\n",
    "        filter_pos = filter_pos + 1\n",
    "\n",
    "\n",
    "def initial_diversity(stats_result, filter_config):\n",
    "    for cate in filter_config['category']:\n",
    "        stats_result[cate] = 0\n",
    "\n",
    "\n",
    "def category_diversity_logic(filter_result, stats_result, dict_category_id, filter_config):\n",
    "    diversity_count = filter_config['category_diversity_count']\n",
    "    min_category = None\n",
    "    min_category_count = 999\n",
    "    candidate_category_list = []\n",
    "    for cate, count in stats_result.items():\n",
    "        if count < min_category_count and count != 0:\n",
    "            min_category_count = count\n",
    "            min_category = cate\n",
    "        elif count == 0:\n",
    "            candidate_category_list.append(cate)\n",
    "    if min_category != None:\n",
    "        candidate_category_list.append(min_category)\n",
    "    diversity_result_list = []\n",
    "    diversity_result_content_list = []\n",
    "    current_diversity_count = 0\n",
    "\n",
    "    filter_result_list = list(filter_result.keys())\n",
    "    filter_result_content_list = list(filter_result.values())\n",
    "    sample_try = 0\n",
    "    catch_count = 0\n",
    "    while catch_count < diversity_count:\n",
    "        for cate in candidate_category_list:\n",
    "            sample_try = sample_try + 1\n",
    "            candidate_id = sample(dict_category_id[str(cate)], 1)[0]\n",
    "            if candidate_id in filter_result_list:\n",
    "                continue\n",
    "            else:\n",
    "                filter_result_list.append(str(candidate_id))\n",
    "                filter_result_content_list.append([str(candidate_id), 'diversity', 0.0,\n",
    "                                                   'batch_diversity_{}|{}'.format(len(filter_result_list), cate)])\n",
    "                catch_count = catch_count + 1\n",
    "                if catch_count >= diversity_count:\n",
    "                    break\n",
    "        if sample_try > 5 * diversity_count:\n",
    "            logging.error(\n",
    "                \"fail to find enough diversity candidate, need to find {} but only find {}\".format(diversity_count,\n",
    "                                                                                                   catch_count + 1))\n",
    "            break\n",
    "\n",
    "    update_filter_result = dict(zip(filter_result_list, filter_result_content_list))\n",
    "    return update_filter_result\n",
    "\n",
    "\n",
    "# 同一批次去重/统计\n",
    "# 运行时机\n",
    "run_timing = 'batch'\n",
    "dict_filter_result = {}\n",
    "for user_id, recall_result in dict_recall_result.items():\n",
    "    # print(\"user id {}\".format(user_id))\n",
    "    current_user_result = {}\n",
    "    current_diversity_result = {}\n",
    "    initial_diversity(current_diversity_result, filter_config)\n",
    "    for recall_id, recall_property in recall_result.items():\n",
    "        # print(\"item id {}\".format(recall_id))\n",
    "        # print(\"dict rank result {}\".format(dict_rank_result[str(user_id)]))\n",
    "        # 构建recall_type\n",
    "        recall_type = mt_construct(run_timing, recall_property[1], recall_property[2])\n",
    "        # 构建recall_score\n",
    "        recall_score = round(recall_property[3], 2)\n",
    "        # 构建rank_type\n",
    "        rank_pos = str(get_dict_pos(int(recall_id), dict_rank_result[str(user_id)]))\n",
    "        rank_type = mt_construct(run_timing, 'dkn', rank_pos)\n",
    "        # 构建rank_score\n",
    "        rank_score = round(float(dict_rank_result[str(user_id)][str(recall_id)]), 2)\n",
    "        # 构建filter_type\n",
    "        filter_type = mt_construct(run_timing, 'recommend', 'TBD')\n",
    "        # 构建filter_score\n",
    "        filter_score = calc_filter_score(recall_score, rank_score)\n",
    "        #         print(\"{}|{}|{}|{}|{}|{}\".format(recall_type,recall_score,rank_type,rank_score))\n",
    "        #         break\n",
    "        recommend_trace = \"{}|{}|{}|{}|{}|{}\".format(recall_type, recall_score, rank_type, rank_score, filter_type,\n",
    "                                                     filter_score)\n",
    "        current_user_result[str(recall_id)] = []\n",
    "        current_user_result[str(recall_id)].append(str(recall_id))\n",
    "        current_user_result[str(recall_id)].append('recommend')\n",
    "        current_user_result[str(recall_id)].append(filter_score)\n",
    "        current_user_result[str(recall_id)].append(recommend_trace)\n",
    "        # 更新多样性统计\n",
    "        current_category = dict_id_content[str(recall_id)]['type']\n",
    "        for cate in current_category:\n",
    "            if cate is not None:\n",
    "                current_diversity_result[cate] = current_diversity_result[cate] + 1\n",
    "    # 根据filter score更新排序\n",
    "    sort_and_fill_pos(current_user_result)\n",
    "    update_user_result = category_diversity_logic(current_user_result, current_diversity_result, dict_type_id,\n",
    "                                                  filter_config)\n",
    "    dict_filter_result[str(user_id)] = update_user_result\n",
    "\n",
    "file_name = 'info/filter_batch_result.pickle'\n",
    "output_file = open(file_name, 'wb')\n",
    "pickle.dump(dict_filter_result, output_file)\n",
    "output_file.close()\n",
    "\n",
    "write_to_s3(file_name,\n",
    "            bucket,\n",
    "            '{}/feature/recommend-list/news/filter_batch_result.pickle'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k ['6552465493255520771', '6552333627890336263', '6552375889470947847', '6552317622828925454', '6552129817259541000', '6552390995558793735', '6552430823117685252', '6552401718930309639', '6552322115654124046', '6552358557814096388', '6552339837855203843', '6552678274802123268', '6552272755272712717', '6552414390778331661', '6518876067996893447', '6552447020051726862', '6553038669672874504', '6552299905497432584', '6552351206654607880', '6552231406087438852', '6552296494605533703', '6553135697790763523', '6454027220095598862', '6502025540390617358', '6552421326995325444', '6552370042753778183', '6551608303007302151', '6552440072292008451', '6552317622845702669', '6432979375800451330', '6462887973519098126', '6552300151321395725', '6552414856346075662', '6482621460023099662', '6552319161060557319', '6543027763874365966', '6552326434893857284', '6552303886323941891', '6552289068397363716', '6552377718296543757', '6553432442324124163', '6552463126468493838', '6552435868445966855', '6553986522603848200', '6553064211814023688', '6552323431143047683', '6552429618819760643', '6422082903882072321', '6552445705984672260', '6550502136969429507', '6526321960471757316', '6552319871793758723', '6422169653342109954', '6553118061820379655', '6552327554315846152', '6553004967295189517', '6552917997525139982', '6552080356982391309', '6552314385220502024', '6491045467281948941', '6521606799425012232', '6552319214466630147', '6552445705913369102', '6552397522113921549', '6552292813512376845', '6494036844265603341', '6552409228647072269', '6553135703562125837', '6552428222233969165', '6552236533598913032', '6552473959202292232', '6552017667341943304', '6552437290633789956', '6552268740111630861', '6552395503819031043', '6552273880004690436', '6552267310592164355', '6553046350680818180', '6552070854790873603', '6552016317312926222', '6552299951060156942', '6491140579710206221', '6552444517297947140', '6552372557402604036', '6553395790486700548', '6552373629689004557', '6553017364521157127', '6552817381255676424', '6525341647792767502', '6552345615357968899', '6553102412159123972', '6552388108204114435', '6551982425868599811', '6552367250232312324', '6552243921328538115', '6459890271902499086', '6552415139205743108', '6552015749588713997', '6552404376625873411', '6439210298212614413'] v [{\"'\": '0.042185873'}, {' ': '0.041582167'}, {' ': '0.043948144'}, {' ': '0.035612643'}, {' ': '0.04700917'}, {' ': '0.041946203'}, {' ': '0.03754565'}, {' ': '0.03954491'}, {' ': '0.0396401'}, {' ': '0.04353249'}, {' ': '0.04072079'}, {' ': '0.03955552'}, {' ': '0.04517299'}, {' ': '0.036337525'}, {' ': '0.039859414'}, {' ': '0.04364407'}, {' ': '0.036983013'}, {' ': '0.03470421'}, {' ': '0.04074508'}, {' ': '0.045989662'}, {' ': '0.04385358'}, {' ': '0.036112577'}, {' ': '0.041977793'}, {' ': '0.038128167'}, {' ': '0.040419847'}, {' ': '0.036623925'}, {' ': '0.043361813'}, {' ': '0.03537163'}, {' ': '0.03848052'}, {' ': '0.03631383'}, {' ': '0.042770058'}, {' ': '0.04507771'}, {' ': '0.047006875'}, {' ': '0.0362294'}, {' ': '0.040307075'}, {' ': '0.0415861'}, {' ': '0.039907992'}, {' ': '0.03954172'}, {' ': '0.053114504'}, {' ': '0.040926367'}, {' ': '0.045488805'}, {' ': '0.04179716'}, {' ': '0.04148388'}, {' ': '0.03827116'}, {' ': '0.04242459'}, {' ': '0.036404222'}, {' ': '0.037812024'}, {' ': '0.03749594'}, {' ': '0.042123735'}, {' ': '0.039359987'}, {' ': '0.047051013'}, {' ': '0.05176896'}, {' ': '0.037593186'}, {' ': '0.037744015'}, {' ': '0.040528893'}, {' ': '0.042524964'}, {' ': '0.04111734'}, {' ': '0.035677463'}, {' ': '0.04413122'}, {' ': '0.04604578'}, {' ': '0.041270107'}, {' ': '0.041185796'}, {' ': '0.03792259'}, {' ': '0.04490155'}, {' ': '0.047013313'}, {' ': '0.045920104'}, {' ': '0.049389243'}, {' ': '0.043591917'}, {' ': '0.0458'}, {' ': '0.03999096'}, {' ': '0.04541096'}, {' ': '0.040810943'}, {' ': '0.04551202'}, {' ': '0.04166034'}, {' ': '0.044446617'}, {' ': '0.040740818'}, {' ': '0.042031795'}, {' ': '0.03575772'}, {' ': '0.045725286'}, {' ': '0.03726247'}, {' ': '0.042799324'}, {' ': '0.037516028'}, {' ': '0.036073744'}, {' ': '0.034800887'}, {' ': '0.04094708'}, {' ': '0.040382773'}, {' ': '0.03506151'}, {' ': '0.04062277'}, {' ': '0.046188593'}, {' ': '0.040587187'}, {' ': '0.043694288'}, {' ': '0.046622187'}, {' ': '0.03717193'}, {' ': '0.033804834'}, {' ': '0.04159245'}, {' ': '0.046201617'}, {' ': '0.04332471'}, {' ': '0.049688786'}, {' ': '0.03790596'}, {' ': '0.044611514'}]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k, v in sort_id_score_dict.items():\n",
    "    print(\"k {} v {}\".format(k,v))\n",
    "    if n > 10:\n",
    "        break\n",
    "    n = n + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
