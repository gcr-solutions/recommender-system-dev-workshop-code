{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys path is ['/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python36.zip', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/lib-dynload', '', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/dglke-0.1.0.dev0-py3.6.egg', '/home/ec2-user/workplace/Aws-gcr-sc-recommender-system/src/offline/docker/batch/container/batch/src/fasthan', '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ec2-user/.ipython']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import kg\n",
    "import encoding\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "# tqdm.pandas()\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "# bucket = os.environ.get(\"BUCKET_NAME\", \" \")\n",
    "# raw_data_folder = os.environ.get(\"RAW_DATA\", \" \")\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "# tqdm_notebook().pandas()\n",
    "s3client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=aws-gcr-rs-sol-demo-ap-southeast-1-522244679887\n",
      "prefix='sample-data'\n",
      "file preparation: download src key sample-data/model/rank/content/dkn_embedding_latest/complete_dkn_entity_embedding.npy to dst key info/complete_dkn_entity_embedding.npy\n",
      "file preparation: download src key sample-data/model/rank/content/dkn_embedding_latest/complete_dkn_word_embedding.npy to dst key info/complete_dkn_word_embedding.npy\n",
      "file preparation: download src key sample-data/system/item-data/item.csv to dst key info/item.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:55: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "\n",
    "\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    print(\"upload s3://{}/{}\".format(bucket, key))\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        # return s3client.upload_fileobj(f, bucket, key)\n",
    "        return s3client.put_object(\n",
    "            ACL='bucket-owner-full-control',\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=f\n",
    "        )\n",
    "\n",
    "def write_str_to_s3(content, bucket, key):\n",
    "    print(\"write s3://{}/{}, content={}\".format(bucket, key, content))\n",
    "    s3client.put_object(Body=str(content).encode(\"utf8\"), Bucket=bucket, Key=key, ACL='bucket-owner-full-control')\n",
    "\n",
    "default_bucket = 'aws-gcr-rs-sol-demo-ap-southeast-1-522244679887'\n",
    "default_prefix = 'sample-data'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str)\n",
    "parser.add_argument('--prefix', type=str)\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "prefix = args.prefix\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "out_s3_path = \"s3://{}/{}/feature/content/inverted-list\".format(bucket, prefix)\n",
    "\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "\n",
    "file_name_list = ['complete_dkn_entity_embedding.npy','complete_dkn_word_embedding.npy']\n",
    "s3_folder = '{}/model/rank/content/dkn_embedding_latest/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "        \n",
    "file_name_list = ['item.csv']\n",
    "s3_folder = '{}/system/item-data'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "\n",
    "df_filter_item = pd.read_csv('info/item.csv',sep='_!_',names=['news_id','type_code','type','title','keywords','popularity','new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dkn_word_embed = np.load(\"info/complete_dkn_word_embedding.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kg env: {'GRAPH_BUCKET': 'aws-gcr-rs-sol-demo-ap-southeast-1-522244679887', 'KG_DBPEDIA_KEY': 'sample-data/model/meta_files/kg_dbpedia.txt', 'KG_ENTITY_KEY': 'sample-data/model/meta_files/entities_dbpedia.dict', 'KG_RELATION_KEY': 'sample-data/model/meta_files/relations_dbpedia.dict', 'KG_DBPEDIA_TRAIN_KEY': 'sample-data/model/meta_files/kg_dbpedia_train.txt', 'KG_ENTITY_TRAIN_KEY': 'sample-data/model/meta_files/entities_dbpedia_train.dict', 'KG_RELATION_TRAIN_KEY': 'sample-data/model/meta_files/relations_dbpedia_train.dict', 'KG_ENTITY_INDUSTRY_KEY': 'sample-data/model/meta_files/entity_industry.txt', 'KG_VOCAB_KEY': 'sample-data/model/meta_files/vocab.json', 'DATA_INPUT_KEY': '', 'TRAIN_OUTPUT_KEY': 'sample-data/model/meta_files/'}\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/kg_dbpedia_train.txt\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/kg_dbpedia.txt\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/entities_dbpedia.dict\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/entities_dbpedia_train.dict\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/relations_dbpedia.dict\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/relations_dbpedia_train.dict\n",
      "load file: aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/entity_industry.txt\n",
      "loading vocabulary file /home/ec2-user/.fastNLP/fasthan/fasthan_base/vocab.txt\n",
      "Load pre-trained BERT parameters from file /home/ec2-user/.fastNLP/fasthan/fasthan_base/model.bin.\n"
     ]
    }
   ],
   "source": [
    "# prepare model for batch process\n",
    "meta_file_prefix = \"{}/model/meta_files\".format(prefix)\n",
    "os.environ['GRAPH_BUCKET'] = bucket\n",
    "os.environ['KG_DBPEDIA_KEY'] = '{}/kg_dbpedia.txt'.format(meta_file_prefix)\n",
    "os.environ['KG_ENTITY_KEY'] = '{}/entities_dbpedia.dict'.format(meta_file_prefix)\n",
    "os.environ['KG_RELATION_KEY'] = '{}/relations_dbpedia.dict'.format(meta_file_prefix)\n",
    "os.environ['KG_DBPEDIA_TRAIN_KEY'] = '{}/kg_dbpedia_train.txt'.format(meta_file_prefix)\n",
    "os.environ['KG_ENTITY_TRAIN_KEY'] = '{}/entities_dbpedia_train.dict'.format(meta_file_prefix)\n",
    "os.environ['KG_RELATION_TRAIN_KEY'] = '{}/relations_dbpedia_train.dict'.format(meta_file_prefix)\n",
    "os.environ['KG_ENTITY_INDUSTRY_KEY'] = '{}/entity_industry.txt'.format(meta_file_prefix)\n",
    "os.environ['KG_VOCAB_KEY'] = '{}/vocab.json'.format(meta_file_prefix)\n",
    "os.environ['DATA_INPUT_KEY'] = ''\n",
    "os.environ['TRAIN_OUTPUT_KEY'] = '{}/'.format(meta_file_prefix)\n",
    "\n",
    "kg_path = os.environ['GRAPH_BUCKET']\n",
    "dbpedia_key = os.environ['KG_DBPEDIA_KEY']\n",
    "entity_key = os.environ['KG_ENTITY_KEY']\n",
    "relation_key = os.environ['KG_RELATION_KEY']\n",
    "dbpedia_train_key = os.environ['KG_DBPEDIA_TRAIN_KEY']\n",
    "entity_train_key = os.environ['KG_ENTITY_TRAIN_KEY']\n",
    "relation_train_key = os.environ['KG_RELATION_TRAIN_KEY']\n",
    "entity_industry_key = os.environ['KG_ENTITY_INDUSTRY_KEY']\n",
    "vocab_key = os.environ['KG_VOCAB_KEY']\n",
    "data_input_key = os.environ['DATA_INPUT_KEY']\n",
    "train_output_key = os.environ['TRAIN_OUTPUT_KEY']\n",
    "\n",
    "env = {\n",
    "    'GRAPH_BUCKET': kg_path,\n",
    "    'KG_DBPEDIA_KEY': dbpedia_key,\n",
    "    'KG_ENTITY_KEY': entity_key,\n",
    "    'KG_RELATION_KEY': relation_key,\n",
    "    'KG_DBPEDIA_TRAIN_KEY': dbpedia_train_key,\n",
    "    'KG_ENTITY_TRAIN_KEY': entity_train_key,\n",
    "    'KG_RELATION_TRAIN_KEY': relation_train_key,\n",
    "    'KG_ENTITY_INDUSTRY_KEY': entity_industry_key,\n",
    "    'KG_VOCAB_KEY': vocab_key,\n",
    "    'DATA_INPUT_KEY': data_input_key,\n",
    "    'TRAIN_OUTPUT_KEY': train_output_key\n",
    "}\n",
    "\n",
    "print(\"Kg env: {}\".format(env))\n",
    "graph = kg.Kg(env)  # Where we keep the model when it's loaded\n",
    "model = encoding.encoding(graph, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_id_news_feature_dict = {}\n",
    "map_words = {}\n",
    "map_entities = {}\n",
    "\n",
    "def analyze_map(raw_idx, map_dict, filter_idx):\n",
    "    for idx in raw_idx:\n",
    "        if idx == 0:\n",
    "            filter_idx.append(0)\n",
    "        else:\n",
    "            if idx not in map_dict.keys():\n",
    "                map_dict[idx] = len(map_dict)+1\n",
    "            filter_idx.append(map_dict[idx])\n",
    "\n",
    "for row in df_filter_item.iterrows():\n",
    "    item_row = row[1]\n",
    "    program_id = str(item_row['news_id'])\n",
    "    title_result = model[item_row['title']]\n",
    "    current_words = title_result[0]\n",
    "    current_entities = title_result[1]\n",
    "    filter_words = []\n",
    "    filter_entities = []\n",
    "    analyze_map(current_words, map_words, filter_words)\n",
    "    analyze_map(current_entities, map_entities, filter_entities)\n",
    "    # filter entities & filter words\n",
    "    program_dict = {\n",
    "        'entities': filter_entities,\n",
    "        'words': filter_words\n",
    "    }\n",
    "    news_id_news_feature_dict[program_id] = program_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data for graph train\n",
    "path = '/home/ec2-user/workplace/recommender-system-solution/src/offline/news/item-feature-update-batch/aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files'\n",
    "entities_dbpedia = os.path.join(path,'entities_dbpedia.dict')\n",
    "relations_dbpedia = os.path.join(path,'relations_dbpedia.dict')\n",
    "kg_dbpedia = os.path.join(path, 'kg_dbpedia.txt')\n",
    "entities_dbpedia_train_path = os.path.join(path,'entities_dbpedia_train.dict')\n",
    "relations_dbpedia_train_path = os.path.join(path,'relations_dbpedia_train.dict')\n",
    "kg_dbpedia_train_path = os.path.join(path, 'kg_dbpedia_train.txt')\n",
    "entities_dbpedia_f = pd.read_csv(entities_dbpedia, header=None, names=['e','e_name'])\n",
    "relations_dbpedia_f = pd.read_csv(relations_dbpedia, header=None, names=['e','e_name']) \n",
    "kg_dbpedia_f = pd.read_csv(kg_dbpedia, delimiter='\\t', header=None, names=['h','r','t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_entities -> train_entites\n",
    "# constrcut from entites:\n",
    "entities_dbpedia_slim = {}\n",
    "relations_dbpedia_slim = {}\n",
    "\n",
    "entities_dbpedia_train = {}\n",
    "relations_dbpedia_train = {}\n",
    "\n",
    "entities_dbpedia_train[0] = '0'\n",
    "relations_dbpedia_train[0] = '0'\n",
    "\n",
    "new_list_kg = []\n",
    "\n",
    "def analyze_map_hrt(idx, map_dict, raw_content, train_dict):\n",
    "    # 原始实体从0开始，所以需要归位进行寻找\n",
    "    idx_test = idx - 1\n",
    "    if idx_test not in map_dict.keys():\n",
    "        map_dict[idx_test] = len(map_dict)+1\n",
    "        filter_content = raw_content[raw_content.e == idx_test]\n",
    "        train_dict[len(map_dict)] = filter_content['e_name'].values[0]\n",
    "    return map_dict[idx_test]\n",
    "\n",
    "for raw_entity, new_idx in map_entities.items():\n",
    "    entity_id = raw_entity\n",
    "    map_head_id = analyze_map_hrt(entity_id, entities_dbpedia_slim, entities_dbpedia_f, entities_dbpedia_train)\n",
    "    \n",
    "    kg_found_pd = kg_dbpedia_f[kg_dbpedia_f.h == entity_id]\n",
    "#     print(kg_found_pd)\n",
    "    for found_row in kg_found_pd.iterrows():\n",
    "        relation_id = found_row[1]['r']\n",
    "        tail_id = found_row[1]['t']\n",
    "        map_relation_id = analyze_map_hrt(relation_id, relations_dbpedia_slim, relations_dbpedia_f, relations_dbpedia_train)\n",
    "        map_tail_id = analyze_map_hrt(tail_id, entities_dbpedia_slim, entities_dbpedia_f, entities_dbpedia_train)\n",
    "        # create new kg : h-r-t\n",
    "        kg_row = {}\n",
    "        kg_row['h'] = map_head_id\n",
    "        kg_row['r'] = map_relation_id\n",
    "        kg_row['t'] = map_tail_id        \n",
    "        new_list_kg.append(kg_row)\n",
    "\n",
    "kg_dbpedia_slim = pd.DataFrame(new_list_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_dbpedia_slim.to_csv(kg_dbpedia_train_path, sep='\\t', header=False, index=False)\n",
    "import csv\n",
    "\n",
    "with open(entities_dbpedia_train_path, 'w') as f:\n",
    "    for key in entities_dbpedia_train.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,entities_dbpedia_train[key]))\n",
    "        \n",
    "with open(relations_dbpedia_train_path, 'w') as f:\n",
    "    for key in relations_dbpedia_train.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,relations_dbpedia_train[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slim version\n",
    "list_word_embedding = []\n",
    "list_word_embedding.append([0]*300)\n",
    "for raw_key, map_v in map_words.items(): \n",
    "    list_word_embedding.append(complete_dkn_word_embed[raw_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/kg_dbpedia_train.txt\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/entities_dbpedia_train.dict\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/meta_files/relations_dbpedia_train.dict\n",
      "upload s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/feature/content/inverted-list/news_id_news_feature_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "file_name = 'info/dkn_word_embedding.npy'\n",
    "with open(file_name, \"wb\") as f:\n",
    "    np.save(f, np.array(list_word_embedding))\n",
    "\n",
    "write_to_s3(file_name,\n",
    "            bucket,\n",
    "            '{}/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy'.format(prefix))\n",
    "\n",
    "write_to_s3(kg_dbpedia_train_path,\n",
    "            bucket,\n",
    "            '{}/kg_dbpedia_train.txt'.format(meta_file_prefix))\n",
    "\n",
    "write_to_s3(entities_dbpedia_train_path,\n",
    "            bucket,\n",
    "            '{}/entities_dbpedia_train.dict'.format(meta_file_prefix))\n",
    "\n",
    "write_to_s3(relations_dbpedia_train_path,\n",
    "            bucket,\n",
    "            '{}/relations_dbpedia_train.dict'.format(meta_file_prefix))\n",
    "\n",
    "file_name = 'info/news_id_news_feature_dict.pickle'\n",
    "out_file = open(file_name, 'wb')\n",
    "pickle.dump(news_id_news_feature_dict, out_file)\n",
    "out_file.close()\n",
    "# s3_url = S3Uploader.upload(file_name, out_s3_path)\n",
    "s3_url = write_to_s3(file_name, bucket, '{}/feature/content/inverted-list/news_id_news_feature_dict.pickle'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy to s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/complete_dkn_word_embedding.npy\n",
      "copy: s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy to s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/complete_dkn_entity_embedding.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_word_embedding.npy s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/complete_dkn_word_embedding.npy  --acl bucket-owner-full-control\n",
    "!aws s3 cp s3://gcr-rs-ops-ap-southeast-1-522244679887/news-open/model/rank/content/dkn_embedding_latest/dkn_entity_embedding.npy s3://aws-gcr-rs-sol-demo-ap-southeast-1-522244679887/sample-data/model/rank/content/dkn_embedding_latest/complete_dkn_entity_embedding.npy  --acl bucket-owner-full-control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 6423135627981619458 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0], 'words': [1, 2, 0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 0, 0]}\n",
      "k 6481778451320668430 v {'entities': [3, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [13, 0, 14, 15, 16, 17, 14, 18, 19, 20, 21, 22, 23, 18, 24, 0]}\n",
      "k 6487781016990646541 v {'entities': [0, 0, 5, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0], 'words': [25, 26, 27, 28, 29, 30, 31, 32, 31, 33, 34, 35, 36, 37, 0, 38]}\n",
      "k 6492610374146195725 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0], 'words': [39, 40, 32, 17, 41, 42, 43, 17, 44, 45, 2, 40, 31, 32, 31, 0]}\n",
      "k 6512940160806551816 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [46, 47, 48, 49, 50, 51, 52, 53, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6520157116831891716 v {'entities': [0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [54, 55, 56, 0, 57, 26, 58, 59, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "k 6520548609547567367 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [60, 61, 62, 63, 64, 65, 66, 67, 61, 31, 68, 69, 70, 50, 71, 72]}\n",
      "k 6521960096736477700 v {'entities': [0, 0, 9, 0, 10, 0, 0, 11, 0, 12, 0, 0, 0, 0, 0, 0], 'words': [73, 74, 75, 76, 77, 78, 74, 79, 76, 80, 81, 74, 82, 83, 84, 85]}\n",
      "k 6528436786643862029 v {'entities': [0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [86, 87, 0, 44, 88, 89, 90, 91, 0, 92, 93, 94, 95, 26, 96, 0]}\n",
      "k 6531118183804305924 v {'entities': [0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [97, 17, 98, 29, 99, 26, 100, 101, 102, 103, 104, 43, 105, 106, 0, 0]}\n",
      "k 6537451193822609923 v {'entities': [0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [107, 108, 109, 110, 111, 108, 112, 113, 70, 114, 115, 116, 117, 118, 119, 120]}\n",
      "k 6539681813122515469 v {'entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'words': [99, 121, 122, 123, 124, 69, 26, 0, 125, 126, 127, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k, v in news_id_news_feature_dict.items():\n",
    "    print(\"k {} v {}\".format(k,v))\n",
    "    if n > 10:\n",
    "        break\n",
    "    n = n + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
