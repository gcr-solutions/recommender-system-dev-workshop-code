{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm info/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepfm模型训练逻辑\n",
    "# 基础依赖\n",
    "import math\n",
    "import argparse\n",
    "import pickle\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import deepfm\n",
    "import random\n",
    "# 模型相关\n",
    "from tensorflow.python.keras.models import Model, save_model, load_model\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat,get_feature_names\n",
    "from deepmatch.layers import custom_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=sagemaker-us-east-1-002224604296\n",
      "prefix='recommender-system-film-mk/1'\n",
      "file preparation: download src key recommender-system-film-mk/1/system/user-data/clean/latest/action.csv to dst key info/action.csv\n",
      "file preparation: download src key recommender-system-film-mk/1/feature/action/raw_embed_item_mapping.pickle to dst key info/raw_embed_item_mapping.pickle\n",
      "file preparation: download src key recommender-system-film-mk/1/feature/action/raw_embed_user_mapping.pickle to dst key info/raw_embed_user_mapping.pickle\n",
      "file preparation: download src key recommender-system-film-mk/1/model/recall/youtubednn/user_embeddings.h5 to dst key info/user_embeddings.h5\n",
      "file preparation: download src key recommender-system-film-mk/1/feature/content/inverted-list/movie_id_movie_feature_dict.pickle to dst key info/movie_id_movie_feature_dict.pickle\n",
      "load 207341 action data\n",
      "length of movie_id v.s. movie_property 33767\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/deepctr/layers/utils.py:167: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/deepctr/layers/utils.py:199: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 从s3同步数据\n",
    "########################################\n",
    "def sync_s3(file_name_list, s3_folder, local_folder):\n",
    "    for f in file_name_list:\n",
    "        print(\"file preparation: download src key {} to dst key {}\".format(os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f)))\n",
    "        s3client.download_file(bucket, os.path.join(\n",
    "            s3_folder, f), os.path.join(local_folder, f))\n",
    "        \n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename, 'rb') as f:  # Read in binary mode\n",
    "        return s3client.upload_fileobj(f, bucket, key)\n",
    "\n",
    "default_bucket = 'sagemaker-us-east-1-002224604296'\n",
    "default_mk_region = '1'\n",
    "level_1 = 'recommender-system-film-mk'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bucket', type=str, default=default_bucket)\n",
    "parser.add_argument('--mk-region', type=str, default=default_mk_region)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "bucket = args.bucket\n",
    "mk_region = args.mk_region\n",
    "\n",
    "prefix = f\"{level_1}/{mk_region}\"\n",
    "\n",
    "print(\"bucket={}\".format(bucket))\n",
    "print(\"prefix='{}'\".format(prefix))\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "local_folder = 'info'\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "# 行为数据加载\n",
    "file_name_list = ['action.csv']\n",
    "s3_folder = '{}/system/user-data/clean/latest'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# youtubednn模型数据加载\n",
    "file_name_list = ['raw_embed_item_mapping.pickle', 'raw_embed_user_mapping.pickle']\n",
    "s3_folder = '{}/feature/action/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "file_name_list = ['user_embeddings.h5']\n",
    "s3_folder = 'recommender-system-film-mk/1/model/recall/youtubednn/'\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "# 倒排列表的pickle文件\n",
    "file_name_list = ['movie_id_movie_feature_dict.pickle']\n",
    "s3_folder = '{}/feature/content/inverted-list/'.format(prefix)\n",
    "sync_s3(file_name_list, s3_folder, local_folder)\n",
    "\n",
    "# 加载所有人的数据\n",
    "action_data_pddf = pd.read_csv(\"info/action.csv\", sep='\\t')\n",
    "print(\"load {} action data\".format(len(action_data_pddf)))\n",
    "# 加载pickle文件\n",
    "file_to_load = open(\"info/movie_id_movie_feature_dict.pickle\", \"rb\")\n",
    "dict_id_feature_pddf = pd.read_pickle(file_to_load)\n",
    "print(\"length of movie_id v.s. movie_property {}\".format(len(dict_id_feature_pddf)))\n",
    "file_to_load = open(\"info/raw_embed_item_mapping.pickle\", \"rb\")\n",
    "raw_embed_item_mapping = pickle.load(file_to_load)\n",
    "file_to_load = open(\"info/raw_embed_user_mapping.pickle\", \"rb\")\n",
    "raw_embed_user_mapping = pickle.load(file_to_load)\n",
    "# 加载模型\n",
    "user_embedding_model = load_model('info/user_embeddings.h5', custom_objects)\n",
    "embed_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1802/1802 [00:00<00:00, 4025.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>userid</th>\n",
       "      <th>programId</th>\n",
       "      <th>timeStamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13396</td>\n",
       "      <td>1690098</td>\n",
       "      <td>1608198651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13396</td>\n",
       "      <td>1690115</td>\n",
       "      <td>1608198651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>13396</td>\n",
       "      <td>1690113</td>\n",
       "      <td>1608198653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>13396</td>\n",
       "      <td>1690097</td>\n",
       "      <td>1608198653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13396</td>\n",
       "      <td>1690115</td>\n",
       "      <td>1608198657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label userid  programId   timeStamp\n",
       "0      0  13396    1690098  1608198651\n",
       "1      1  13396    1690115  1608198651\n",
       "2      0  13396    1690113  1608198653\n",
       "3      0  13396    1690097  1608198653\n",
       "4      1  13396    1690115  1608198657"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_pddf = action_data_pddf[action_data_pddf['programType']==1][['label','userid','programId','timeStamp']]\n",
    "sample_data_pddf.sort_values('timeStamp',inplace=True)\n",
    "\n",
    "sample_data_pddf_click = sample_data_pddf[sample_data_pddf['label']==1]\n",
    "user_click_record = {}\n",
    "for reviewerID, hist in tqdm(sample_data_pddf_click.groupby('userid')):\n",
    "    current_user_time_list = {}\n",
    "    pos_list = hist['programId'].tolist()\n",
    "    time_list = hist['timeStamp'].tolist()\n",
    "    for idx, t in enumerate(time_list):\n",
    "        current_user_time_list[t] = pos_list[idx:]\n",
    "    user_click_record[str(reviewerID)] = current_user_time_list\n",
    "\n",
    "sample_data_pddf = sample_data_pddf.drop_duplicates()\n",
    "sample_data_pddf = sample_data_pddf.reset_index(drop=True)\n",
    "sample_data_pddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_feat(x, i):\n",
    "    return x[i]\n",
    "#     return pd.Series(f_dict)\n",
    "\n",
    "def min_max_norm(raw_list):\n",
    "#     current_mean = np.mean(raw_list)\n",
    "    min_max_norm = [((float)(i)-min(raw_list))/(max(raw_list)-min(raw_list)) for i in raw_list]\n",
    "    return min_max_norm\n",
    "#     if math.isclose(current_mean,0) or math.isclose(mean_value,0):\n",
    "#         return raw_list\n",
    "#     else:\n",
    "#         raw_list = current_meanfloat(mean_value) * raw_list\n",
    "#         return raw_list\n",
    "\n",
    "def user_embed_func(x, user_click_record=user_click_record, user_embedding_model=user_embedding_model, dict_item_mapping=raw_embed_item_mapping, dict_user_mapping=raw_embed_user_mapping):\n",
    "    current_time_stamp = x['timeStamp']\n",
    "    if str(x['userid']) in user_click_record.keys():\n",
    "        current_click_record = user_click_record[str(x['userid'])]\n",
    "#         last_ts = list(current_click_record.keys())[0]\n",
    "#         print(\"current stamp is {}\".format(current_time_stamp))\n",
    "        input_item_list = []\n",
    "        for ts in current_click_record.keys():\n",
    "            if current_time_stamp < ts:\n",
    "#                 print(\"found last ts {}\".format(ts))\n",
    "                input_item_list = current_click_record[ts]\n",
    "                break\n",
    "        # get embedding value\n",
    "        user_id = x['userid']\n",
    "        watch_list_len = 50\n",
    "        map_input_item_list = np.array([[0]*watch_list_len])\n",
    "        watch_len = len(input_item_list)\n",
    "        map_user_id = dict_user_mapping[str(user_id)]\n",
    "        for cnt, item in enumerate(input_item_list):\n",
    "            if cnt < 50:\n",
    "                map_input_item_list[0][cnt] = dict_item_mapping[str(item)]\n",
    "        model_input = {}\n",
    "        model_input['user_id'] = np.array([int(map_user_id)])\n",
    "        model_input['hist_movie_id'] = map_input_item_list\n",
    "        model_input['hist_len'] = np.array([watch_len])\n",
    "\n",
    "        # 更新用户的embeddings\n",
    "    #     print(\"model input {}\".format(model_input))\n",
    "        updated_user_embs = user_embedding_model.predict(\n",
    "            model_input, batch_size=2 ** 12)\n",
    "        # 做归一化\n",
    "        return min_max_norm(updated_user_embs[0])\n",
    "    else:\n",
    "#         print(\"zero click!!\")\n",
    "        return [0]*embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_feature_pddf['programId'] = dict_id_feature_pddf['programId'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_pddf = pd.merge(left=sample_data_pddf, right=dict_id_feature_pddf.drop_duplicates(), how='left', left_on='programId', right_on='programId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据user_id的历史记录生成userid_feat（嵌入）\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab9d189601a40c081de32e266ab81b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200884.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:00<00:03,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "将32维用户嵌入转化为不同的连续型feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:04<00:00,  7.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# user id feature - user embedding\n",
    "print(\"根据user_id的历史记录生成userid_feat（嵌入）\")\n",
    "sample_data_pddf['userid_feat'] = sample_data_pddf.progress_apply(user_embed_func, axis=1)\n",
    "\n",
    "print(\"将{}维用户嵌入转化为不同的连续型feature\".format(embed_dim))\n",
    "for i in tqdm(range(embed_dim)):\n",
    "    sample_data_pddf['user_feature_{}'.format(i)] = sample_data_pddf['userid_feat'].apply(lambda x: user_id_feat(x, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_data = sample_data_pddf\n",
    "dense_feature_size = embed_dim\n",
    "sparse_feature_size = 6\n",
    "for i in range(dense_feature_size):\n",
    "    if i < embed_dim:\n",
    "        mk_data['I{}'.format(i+1)] = mk_data['user_feature_{}'.format(i)]\n",
    "    \n",
    "mk_sparse_features = ['C' + str(i)for i in range(1, sparse_feature_size+1)]\n",
    "mk_dense_features = ['I'+str(i) for i in range(1, dense_feature_size+1)]\n",
    "mk_data[mk_sparse_features] = mk_data[mk_sparse_features].fillna('-1', )\n",
    "mk_data[mk_dense_features] = mk_data[mk_dense_features].fillna(0,)\n",
    "\n",
    "mk_target = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_train, mk_test = train_test_split(mk_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_features = range(1, dense_feature_size+1)\n",
    "categorial_features = range(1, sparse_feature_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './'\n",
    "with open(output_dir + 'tr.libsvm', 'w') as out_train:\n",
    "    with open(output_dir + 'va.libsvm', 'w') as out_valid:\n",
    "        for row in mk_train.iterrows():\n",
    "            item_row = row[1]\n",
    "\n",
    "            feat_vals = []\n",
    "            for i in range(0, len(continous_features)):\n",
    "                feat_vals.append(\n",
    "                    str(continous_features[i]) + ':' + \"{0:.6f}\".format(item_row[mk_dense_features[i]]))\n",
    "\n",
    "            for i in range(0, len(categorial_features)):\n",
    "#                 val = dicts.gen(i, features[mk_sparse_features[i]]) + categorial_feature_offset[i]\n",
    "                feat_vals.append(str(item_row[mk_sparse_features[i]]) + ':1')\n",
    "\n",
    "            label = item_row['label']\n",
    "            if random.randint(0, 9999) % 10 != 0:\n",
    "                out_train.write(\"{0} {1}\\n\".format(label, ' '.join(feat_vals)))\n",
    "            else:\n",
    "                out_valid.write(\"{0} {1}\\n\".format(label, ' '.join(feat_vals)))\n",
    "\n",
    "                \n",
    "with open(output_dir + 'te.libsvm', 'w') as out_test:\n",
    "    for row in mk_test.iterrows():\n",
    "        item_row = row[1]\n",
    "\n",
    "        feat_vals = []\n",
    "        for i in range(0, len(continous_features)):\n",
    "            feat_vals.append(\n",
    "                str(continous_features[i]) + ':' + \"{0:.6f}\".format(item_row[mk_dense_features[i]]))\n",
    "\n",
    "        for i in range(0, len(categorial_features)):\n",
    "#                 val = dicts.gen(i, features[mk_sparse_features[i]]) + categorial_feature_offset[i]\n",
    "            feat_vals.append(str(item_row[mk_sparse_features[i]]) + ':1')\n",
    "\n",
    "        label = item_row['label']\n",
    "        out_test.write(\"{0} {1}\\n\".format(label, ' '.join(feat_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:504: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:504: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:505: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "['deepfm.py', '--data_dir', './', '--servable_model_dir', 'serve', '--log_steps', '10', '--num_epochs', '1', '--field_size', '38', '--feature_size', '117581', '--deep_layers', '2,2,2']\n",
      "task_type  train\n",
      "model_dir  \n",
      "data_dir  ./\n",
      "dt_dir  20210324\n",
      "num_epochs  1\n",
      "feature_size  117581\n",
      "field_size  38\n",
      "embedding_size  32\n",
      "batch_size  64\n",
      "deep_layers  2,2,2\n",
      "dropout  0.5,0.5,0.5\n",
      "loss_type  log_loss\n",
      "optimizer  Adam\n",
      "learning_rate  0.0005\n",
      "batch_norm_decay  0.9\n",
      "batch_norm  False\n",
      "l2_reg  0.0001\n",
      ".//tr*libsvm\n",
      "tr_files: ['./tr.libsvm']\n",
      "va_files: ['./va.libsvm']\n",
      "te_files: ['./te.libsvm']\n",
      "WARNING:tensorflow:From deepfm.py:419: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0325 07:35:42.667393 140202414319424 module_wrapper.py:139] From deepfm.py:419: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "input model dir is  \n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpd0ks5ry8\n",
      "W0325 07:35:42.668226 140202414319424 estimator.py:1821] Using temporary folder as model directory: /tmp/tmpd0ks5ry8\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpd0ks5ry8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f830b86ccf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0325 07:35:42.668538 140202414319424 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpd0ks5ry8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f830b86ccf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0325 07:35:42.673989 140202414319424 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "Parsing ['./tr.libsvm']\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_number is deprecated. Please use tf.strings.to_number instead.\n",
      "\n",
      "W0325 07:35:42.752092 140202414319424 module_wrapper.py:139] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_number is deprecated. Please use tf.strings.to_number instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:115: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0325 07:35:42.887196 140202414319424 deprecation.py:323] From deepfm.py:115: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0325 07:35:42.903506 140202414319424 estimator.py:1148] Calling model_fn.\n",
      "WARNING:tensorflow:From deepfm.py:163: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0325 07:35:42.903733 140202414319424 module_wrapper.py:139] From deepfm.py:163: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:174: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0325 07:35:42.922479 140202414319424 module_wrapper.py:139] From deepfm.py:174: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0325 07:35:42.937671 140202414319424 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0325 07:35:42.939473 140202414319424 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From deepfm.py:215: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0325 07:35:42.956798 140202414319424 deprecation.py:506] From deepfm.py:215: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From deepfm.py:232: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n",
      "W0325 07:35:43.037142 140202414319424 module_wrapper.py:139] From deepfm.py:232: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0325 07:35:43.038656 140202414319424 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From deepfm.py:247: The name tf.metrics.auc is deprecated. Please use tf.compat.v1.metrics.auc instead.\n",
      "\n",
      "W0325 07:35:43.048244 140202414319424 module_wrapper.py:139] From deepfm.py:247: The name tf.metrics.auc is deprecated. Please use tf.compat.v1.metrics.auc instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0325 07:35:43.118018 140202414319424 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From deepfm.py:258: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0325 07:35:43.153981 140202414319424 module_wrapper.py:139] From deepfm.py:258: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:268: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "W0325 07:35:43.154242 140202414319424 module_wrapper.py:139] From deepfm.py:268: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0325 07:35:43.531780 140202414319424 estimator.py:1150] Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "I0325 07:35:43.533320 140202414319424 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0325 07:35:43.853595 140202414319424 monitored_session.py:240] Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0325 07:35:44.701723 140202414319424 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0325 07:35:44.727068 140202414319424 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpd0ks5ry8/model.ckpt.\n",
      "I0325 07:35:45.202728 140202414319424 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /tmp/tmpd0ks5ry8/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7010573, step = 0\n",
      "I0325 07:35:45.789298 140202414319424 basic_session_run_hooks.py:262] loss = 0.7010573, step = 0\n",
      "INFO:tensorflow:global_step/sec: 107.677\n",
      "I0325 07:35:46.717160 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 107.677\n",
      "INFO:tensorflow:loss = 0.42467976, step = 100 (0.929 sec)\n",
      "I0325 07:35:46.718107 140202414319424 basic_session_run_hooks.py:260] loss = 0.42467976, step = 100 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.367\n",
      "I0325 07:35:47.340734 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 160.367\n",
      "INFO:tensorflow:loss = 0.29836518, step = 200 (0.624 sec)\n",
      "I0325 07:35:47.341825 140202414319424 basic_session_run_hooks.py:260] loss = 0.29836518, step = 200 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.177\n",
      "I0325 07:35:47.953589 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 163.177\n",
      "INFO:tensorflow:loss = 0.25469545, step = 300 (0.613 sec)\n",
      "I0325 07:35:47.954699 140202414319424 basic_session_run_hooks.py:260] loss = 0.25469545, step = 300 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.292\n",
      "I0325 07:35:48.605956 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 153.292\n",
      "INFO:tensorflow:loss = 0.29925758, step = 400 (0.655 sec)\n",
      "I0325 07:35:48.609413 140202414319424 basic_session_run_hooks.py:260] loss = 0.29925758, step = 400 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.965\n",
      "I0325 07:35:49.255407 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 153.965\n",
      "INFO:tensorflow:loss = 0.15769653, step = 500 (0.647 sec)\n",
      "I0325 07:35:49.256556 140202414319424 basic_session_run_hooks.py:260] loss = 0.15769653, step = 500 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.841\n",
      "I0325 07:35:49.901273 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 154.841\n",
      "INFO:tensorflow:loss = 0.19700605, step = 600 (0.646 sec)\n",
      "I0325 07:35:49.902781 140202414319424 basic_session_run_hooks.py:260] loss = 0.19700605, step = 600 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.385\n",
      "I0325 07:35:50.540673 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 156.385\n",
      "INFO:tensorflow:loss = 0.2448525, step = 700 (0.639 sec)\n",
      "I0325 07:35:50.541717 140202414319424 basic_session_run_hooks.py:260] loss = 0.2448525, step = 700 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.834\n",
      "I0325 07:35:51.199338 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 151.834\n",
      "INFO:tensorflow:loss = 0.19472815, step = 800 (0.659 sec)\n",
      "I0325 07:35:51.200609 140202414319424 basic_session_run_hooks.py:260] loss = 0.19472815, step = 800 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.483\n",
      "I0325 07:35:51.850872 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 153.483\n",
      "INFO:tensorflow:loss = 0.13249512, step = 900 (0.651 sec)\n",
      "I0325 07:35:51.852083 140202414319424 basic_session_run_hooks.py:260] loss = 0.13249512, step = 900 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.973\n",
      "I0325 07:35:52.504589 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 152.973\n",
      "INFO:tensorflow:loss = 0.11045618, step = 1000 (0.655 sec)\n",
      "I0325 07:35:52.507322 140202414319424 basic_session_run_hooks.py:260] loss = 0.11045618, step = 1000 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.029\n",
      "I0325 07:35:53.137378 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 158.029\n",
      "INFO:tensorflow:loss = 0.1753845, step = 1100 (0.631 sec)\n",
      "I0325 07:35:53.138317 140202414319424 basic_session_run_hooks.py:260] loss = 0.1753845, step = 1100 (0.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.228\n",
      "I0325 07:35:53.773381 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 157.228\n",
      "INFO:tensorflow:loss = 0.16003099, step = 1200 (0.638 sec)\n",
      "I0325 07:35:53.775934 140202414319424 basic_session_run_hooks.py:260] loss = 0.16003099, step = 1200 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.407\n",
      "I0325 07:35:54.429549 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 152.407\n",
      "INFO:tensorflow:loss = 0.11004102, step = 1300 (0.655 sec)\n",
      "I0325 07:35:54.430737 140202414319424 basic_session_run_hooks.py:260] loss = 0.11004102, step = 1300 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.113\n",
      "I0325 07:35:55.062000 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 158.113\n",
      "INFO:tensorflow:loss = 0.069801085, step = 1400 (0.633 sec)\n",
      "I0325 07:35:55.063364 140202414319424 basic_session_run_hooks.py:260] loss = 0.069801085, step = 1400 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.384\n",
      "I0325 07:35:55.697385 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 157.384\n",
      "INFO:tensorflow:loss = 0.14426893, step = 1500 (0.636 sec)\n",
      "I0325 07:35:55.698883 140202414319424 basic_session_run_hooks.py:260] loss = 0.14426893, step = 1500 (0.636 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 149.747\n",
      "I0325 07:35:56.365188 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 149.747\n",
      "INFO:tensorflow:loss = 0.26339057, step = 1600 (0.668 sec)\n",
      "I0325 07:35:56.366428 140202414319424 basic_session_run_hooks.py:260] loss = 0.26339057, step = 1600 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.11\n",
      "I0325 07:35:57.018269 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 153.11\n",
      "INFO:tensorflow:loss = 0.17454745, step = 1700 (0.653 sec)\n",
      "I0325 07:35:57.019295 140202414319424 basic_session_run_hooks.py:260] loss = 0.17454745, step = 1700 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.433\n",
      "I0325 07:35:57.665833 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 154.433\n",
      "INFO:tensorflow:loss = 0.18979602, step = 1800 (0.648 sec)\n",
      "I0325 07:35:57.667066 140202414319424 basic_session_run_hooks.py:260] loss = 0.18979602, step = 1800 (0.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.604\n",
      "I0325 07:35:58.300317 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 157.604\n",
      "INFO:tensorflow:loss = 0.066345945, step = 1900 (0.634 sec)\n",
      "I0325 07:35:58.301400 140202414319424 basic_session_run_hooks.py:260] loss = 0.066345945, step = 1900 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.609\n",
      "I0325 07:35:58.959923 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 151.609\n",
      "INFO:tensorflow:loss = 0.34976926, step = 2000 (0.660 sec)\n",
      "I0325 07:35:58.960977 140202414319424 basic_session_run_hooks.py:260] loss = 0.34976926, step = 2000 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.719\n",
      "I0325 07:35:59.627845 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 149.719\n",
      "INFO:tensorflow:loss = 0.1281912, step = 2100 (0.668 sec)\n",
      "I0325 07:35:59.628756 140202414319424 basic_session_run_hooks.py:260] loss = 0.1281912, step = 2100 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.414\n",
      "I0325 07:36:00.263147 140202414319424 basic_session_run_hooks.py:692] global_step/sec: 157.414\n",
      "INFO:tensorflow:loss = 0.075037874, step = 2200 (0.637 sec)\n",
      "I0325 07:36:00.266154 140202414319424 basic_session_run_hooks.py:260] loss = 0.075037874, step = 2200 (0.637 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2262 into /tmp/tmpd0ks5ry8/model.ckpt.\n",
      "I0325 07:36:00.657464 140202414319424 basic_session_run_hooks.py:606] Saving checkpoints for 2262 into /tmp/tmpd0ks5ry8/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.19587953.\n",
      "I0325 07:36:00.804467 140202414319424 estimator.py:371] Loss for final step: 0.19587953.\n",
      "WARNING:tensorflow:From deepfm.py:493: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0325 07:36:00.810745 140202414319424 module_wrapper.py:139] From deepfm.py:493: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From deepfm.py:501: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function has been renamed, use `export_saved_model` instead.\n",
      "W0325 07:36:00.812694 140202414319424 deprecation.py:323] From deepfm.py:501: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function has been renamed, use `export_saved_model` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0325 07:36:00.820662 140202414319424 estimator.py:1148] Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0325 07:36:00.923866 140202414319424 estimator.py:1150] Done calling model_fn.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0325 07:36:00.924142 140202414319424 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "I0325 07:36:00.924490 140202414319424 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "I0325 07:36:00.924628 140202414319424 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I0325 07:36:00.924742 140202414319424 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "I0325 07:36:00.924848 140202414319424 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "I0325 07:36:00.924952 140202414319424 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpd0ks5ry8/model.ckpt-2262\n",
      "I0325 07:36:00.955445 140202414319424 saver.py:1290] Restoring parameters from /tmp/tmpd0ks5ry8/model.ckpt-2262\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "I0325 07:36:00.993676 140202414319424 builder_impl.py:665] Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "I0325 07:36:00.993909 140202414319424 builder_impl.py:460] No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: serve/temp-1616657760/saved_model.pb\n",
      "I0325 07:36:01.055687 140202414319424 builder_impl.py:425] SavedModel written to: serve/temp-1616657760/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!python deepfm.py --data_dir ./ --servable_model_dir serve --log_steps 10 --num_epochs 1 --field_size 38 --feature_size 117581 --deep_layers '2,2,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans = model.predict(mk_test_model_input, batch_size=256)\n",
    "print(\"test LogLoss\", round(log_loss(mk_test[mk_target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(mk_test[mk_target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model.save(\"DeepFM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储/更新模型文件\n",
    "model_name = 'DeepFM'\n",
    "tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n",
    "tar.add(model_name)\n",
    "tar.close()\n",
    "write_to_s3(\"model.tar.gz\", bucket, \"recommender-system-film-mk/1/model/rank/action/deepfm/latest/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r info/*\n",
    "!python model-update-deepfm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-21 10:33:55--  https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.62.51\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.62.51|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-03-21 10:33:56 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
